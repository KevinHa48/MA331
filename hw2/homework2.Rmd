---
title: "**Homework 2**"
author: "Kevin Ha"
date: "September 12, 2021"
output: html_document
---

---

### **Problem 1**

\ 

##### Problem 1.1

Given $p = 0.4$ and $P(N \leq 8.25)$:


```{r}
pbinom(q = 8.25, size = 20, prob = 0.4)
pbinom(q = 8.25, size = 30, prob = 0.4)
pbinom(q = 8.25, size = 50, prob = 0.4)
pbinom(q = 8.25, size = 75, prob = 0.4)
pbinom(q = 8.25, size = 100, prob = 0.4)
```

\

##### Problem 1.2
```{r, echo = FALSE}
laplace_calculate <- function(num_trials) {
  result <- (8.25 - (num_trials * 0.4)) / sqrt(num_trials * 0.4 *(1-0.4))
  return(result)
}
```
Laplace theorem equation: $P(N \leq x) \approx \Phi\bigg(\frac{x - np}{\sqrt{np(1-p)}}\bigg)$

`laplace_calculate` is a function that calculates $\Phi\bigg(\frac{x - np}{\sqrt{np(1-p)}}\bigg)$ and takes a parameter `num_trials`:
```{r}
pnorm(laplace_calculate(num_trials = 20))
pnorm(laplace_calculate(num_trials = 30))
pnorm(laplace_calculate(num_trials = 50))
pnorm(laplace_calculate(num_trials = 75))
pnorm(laplace_calculate(num_trials = 100))
```

\

##### Problem 1.3
```{r}
accurate_vector <- c(0.5955987, 0.09401122, 0.0002305229, 1.826106e-08, 5.431127e-13)
approx_vector <- c(0.5454243, 0.08112525, 0.0003470073, 1.475701e-07, 4.557597e-11)
error_vector <- abs(accurate_vector - approx_vector)
error_vector
```

```{r, echo = FALSE, fig.align = 'center'}
plot(error_vector, main = "Scatter plot of errors for approximations", xlab = 'Index of Error Vector', ylab = 'Absolute Difference')
```

\

##### Problem 1.4
**Based on the scatter plot above, it can be said that the Laplace theorem becomes more accurate to the actual probability as the number of trials increases.
Thus the larger the trials, the better the normal distribution can be used to approximate the binomial distribution.**

\

### **Problem 2**

```{r, echo = FALSE}
sample_sim <- function(size_of_sample, number_of_samples) {
  sam = c()
  sav = c()
  
  for(i in 1:number_of_samples) {
    x = rnorm(size_of_sample, 2, 3)
    sam = c(sam, mean(x))
    sav = c(sav, var(x))
  }
  
  samn = (sam - 2) / sqrt(3^2 / size_of_sample)
  savn = (size_of_sample - 1) * sav / (3 ^ 2)
  plot(density(samn), main = "Density of sample mean")
  plot(density(savn), main = "Density of sample variance")
  sa = cbind(samn, savn)
  plot(sa, main = "Scatter plot of (mean, variance)", xlab = "Normalized sample means", ylab = "Proportionalized sample variances")
}


```

Given $X \sim N(2,3^2)$

\

##### Problem 2.1 - 2.4

For $n = 20$

```{r, echo = FALSE}
sample_sim(20, 100)
```

---

For $n = 30$

```{r, echo = FALSE}
sample_sim(30, 100)
```

---

For $n = 50$

```{r, echo = FALSE}
sample_sim(50, 100)
```

---

For $n = 75$

```{r, echo = FALSE}
sample_sim(75, 100)
```


##### Problem 2.5
From the plots of the sample mean and sample variance made above, we can say that for a SRS, the sample mean follows a **normal distribution** while the sample variance follows a **$\chi^2$ distribution.**

\

##### Problem 2.6
Based on the scatter plots generated from (sample mean, sample variance) it shows that the two have a weak correlation which proves that the two are **mutually independent.**


### **Problem 3**
A binomial distribution can be interpreted alternatively as the sum of $n$ Bernoulli random variables. Each Bernoulli random variable has a mean defined as $p$.
Using this fact, we can state that $Y_1, ..., Y_n$ to be a series of independent Bernoulli random variables with $E(Y_i) = p$

Since the random binomial random variable, $N$ is the sum of these, it can be said that:

$N = Y_1 + ... + Y_n$

To find the expected value of $N$, the expected value of the sum of all Bernoulli random variables must be derived:

$E(N) = E(Y_1 + ... + Y_n)$

However, using the property of expectations $E[X + Y] = E[X] + E[Y]$, the above can be changed to:

$E(N) = E(Y_1) + ... +  E(Y_n)$

Since all the expected values of each Bernoulli random variable is $p$, the equation above is the same as adding $p$ for $n$ times.

$E(N) = p + ... + p = np$

\

### **Problem 4**
```{r, echo = FALSE}
density_of_t <- "$\\frac{\\Gamma (\\frac{n+1}{2})}{\\sqrt{n\\pi}\\Gamma (\\frac{n+1}{2})}(1+\\frac{x^2}{n})^{-(\\frac{n+1}{2})}$"
```

Given that $T \sim \tau_n$, we know that the random variable $T$ has a t-distribution and is thus also a continuous random variable.

To find the expected value of a continous random variable, the following formula will be used: $E[X] = \int_{-\infty}^{\infty} x * f(x) \,dx$

and $f(x)$ will be the probability density function of $T$: $f(x) =$ `r density_of_t`

So, $E[X] = \int_{-\infty}^{\infty} x *$ `r density_of_t` $\,dx$

Simplifying further, $E[X] = \frac{\Gamma (\frac{n+1}{2})}{\sqrt{n\pi}\Gamma (\frac{n+1}{2})}$ $\int_{-\infty}^{\infty} x * (1+\frac{x^2}{n})^{-(\frac{n+1}{2})}$  

Note that the function $f(x) = x * (1+\frac{x^2}{n})^{-(\frac{n+1}{2})}$ is an odd function since $-f(x) = f(-x)$

When taking the integral of an odd function, the result will always be 0.

Thus, $E[X] = \frac{\Gamma (\frac{n+1}{2})}{\sqrt{n\pi}\Gamma (\frac{n+1}{2})} * 0 = 0$


